{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_classifier_using_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNU7DL7Ipkkw"
      },
      "source": [
        "# Fake News Classifier Using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgq3h4pWPxiQ"
      },
      "source": [
        "\"\"\"\n",
        "@Author: Divyansh.Gupta\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2stFKDwCpcuR"
      },
      "source": [
        "# Dataset Download it from [CLICK HERE](https://https://www.kaggle.com/c/fake-news/data#)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "CLHLRXRFQxdt",
        "outputId": "cc11f2fd-d0f6-45a0-d6f8-a9533d68f4d7"
      },
      "source": [
        "data = pd.read_csv(\"train.csv\")\n",
        "data.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orv6MU_2Q2vP"
      },
      "source": [
        "# Drop all Nan Values\n",
        "data = data.dropna()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xPDiI_cRatY"
      },
      "source": [
        "# Drop label variable to get independent variables\n",
        "X = data.drop('label',axis=1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCX6XnaSRjXZ"
      },
      "source": [
        "# Get dependent variable\n",
        "y = data['label']"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm66iZt5RrH_",
        "outputId": "10b4dce6-58aa-4434-c0e5-4325b73e177a"
      },
      "source": [
        "print(\"Shape of independent features: {} and dependent features {}\".format(X.shape,y.shape))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of independent features: (18285, 4) and dependent features (18285,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADuvwWKWSCBg"
      },
      "source": [
        "from keras.layers import Embedding, Dense, LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4rl19NrV6Od"
      },
      "source": [
        "# Vocab Size \n",
        "voc_size = 5000"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLr72_16V8DY"
      },
      "source": [
        "# Creating copy of dataset\n",
        "message = X.copy() # 2 type of copy: Shallow and deep copy(). \n",
        "#Shallow copy the values and object while deep copy copies the reference of the object of value\n",
        "message.reset_index(inplace=True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X74eYIJKWPr4",
        "outputId": "6968fd34-a685-4bd8-d3ff-1893a0d33e14"
      },
      "source": [
        "import nltk # NLP processsing library\n",
        "import re # Regular Expression \n",
        "from nltk.corpus import stopwords, wordnet # Get stopwords, wordnet from nltk\n",
        "nltk.download('stopwords') # Download list of stopwords\n",
        "nltk.download('wordnet') # Download list of wordnet"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-cNxKPptWxv"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Along with the preprocessing, i'll try you to show the difference between stemming and lemmetization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7l7X1pAX_Ix"
      },
      "source": [
        "# Preprocessing and Stemming\n",
        "from nltk.stem.porter import PorterStemmer # Stemming\n",
        "stemm = PorterStemmer() # create object of PorterStemmer\n",
        "corpus=[] # List to add words to create corpus of words\n",
        "for i in range(0,len(message)):\n",
        "  rev = re.sub('[^a-zA-Z]',\" \",message['title'][i]) # Replace all words with space except a-z and A-z\n",
        "  rev = rev.lower() # Lowercase all the text so that \"USA\" and \"usa\" get same index\n",
        "  rev = rev.split() # Split sentences\n",
        "  rev = [stemm.stem(word) for word in rev if word not in stopwords.words('english')] # Doing stemming and removing stopwords\n",
        "  rev = \" \".join(rev) # Join all words to get sentences back\n",
        "  corpus.append(rev) # Appended to corpus"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq2GuSNnZAwR"
      },
      "source": [
        "# Preprocessing and Lemmetization\n",
        "from nltk.stem import WordNetLemmatizer # Lemmetizing\n",
        "lemm = WordNetLemmatizer() # Create object of WordNetLemmatizer\n",
        "corpus1=[] # List to add words to create corpus of words\n",
        "for i in range(0,len(message)):\n",
        "  rev = re.sub('[^a-zA-Z]',\" \",message['title'][i])# Replace all words with space except a-z and A-z\n",
        "  rev = rev.lower()# Lowercase all the text so that \"USA\" and \"usa\" get same index\n",
        "  rev = rev.split()# Split sentences\n",
        "  rev = [lemm.lemmatize(word) for word in rev if word not in stopwords.words('english')]# Doing lemmatizing and removing stopwords\n",
        "  rev = \" \".join(rev)# Join all words to get sentences back\n",
        "  corpus1.append(rev)# Appended to corpus"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GYw-152FZJ3M",
        "outputId": "c82ae271-ddce-4cc1-8bb7-2ca46672b24d"
      },
      "source": [
        "# Sentence after applying stemming\n",
        "corpus[1]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flynn hillari clinton big woman campu breitbart'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y5AUgCCXagZu",
        "outputId": "f9e3618c-86f9-4bab-de63-80a5fffab281"
      },
      "source": [
        "# Sentence after applying Lemmatization\n",
        "corpus1[1]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flynn hillary clinton big woman campus breitbart'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovFcjpJCu-8o"
      },
      "source": [
        "You can see the differences here.\n",
        "lemmatization tried to get meaningful words while in stemming it just generate the root word that can be meaningful or meaningless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKJ7GgrfvWL0"
      },
      "source": [
        "# One Hot Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-0up5AaiEq"
      },
      "source": [
        "# Applying One Hot Encoding\n",
        "one_hot_rep = [one_hot(words, voc_size) for words in corpus]\n",
        "one_hot_rep1 = [one_hot(words1, voc_size) for words1 in corpus1]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Q4rGiTa0Hl",
        "outputId": "c41c27d8-9cdd-434a-c2c2-19ee64cdf110"
      },
      "source": [
        "# one hot representation of first word in corpus after applying stemming\n",
        "one_hot_rep[0]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4711, 2233, 451, 721, 1711, 2815, 4043, 784, 3624, 1750]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlwUWT5aa23Z",
        "outputId": "8fdd89ea-6e3b-4f23-ffb7-c44c7f4e7d63"
      },
      "source": [
        "# one hot representation of first word in corpus after applying lemmatizing\n",
        "one_hot_rep1[0]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1611, 2233, 3487, 721, 1711, 2815, 4043, 784, 3624, 1387]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iss9htqvqcv"
      },
      "source": [
        "# Embedding Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og-guvv7a4gr"
      },
      "source": [
        "sent_len=20 # Max length of sentence\n",
        "# Applying padding so that all vector would be of same length\n",
        "# Cause LSTM always need vectors of same length\n",
        "emb = pad_sequences(one_hot_rep,padding=\"pre\",maxlen=sent_len) \n",
        "emb1 = pad_sequences(one_hot_rep1,padding=\"pre\",maxlen=sent_len)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0UHlzasbUAt",
        "outputId": "e7c37e1c-3c05-4d4d-8b32-d4fa5c663a78"
      },
      "source": [
        "# Embedding representation of first word in corpus after applying stemming\n",
        "emb[0]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 4711,\n",
              "       2233,  451,  721, 1711, 2815, 4043,  784, 3624, 1750], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDC_GKHtbV-f",
        "outputId": "5f444215-23a2-4ebb-ef84-35a87b3fa037"
      },
      "source": [
        "# Embedding representation of first word in corpus after applying lemmatizing\n",
        "emb1[0]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1611,\n",
              "       2233, 3487,  721, 1711, 2815, 4043,  784, 3624, 1387], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZXiD3emxoD1"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lslxa0WubYKn",
        "outputId": "f39e47a6-894f-4263-e72b-34c2f2cc0320"
      },
      "source": [
        "# model\n",
        "emb_vec_feature=40 # Output Vector size\n",
        "model = Sequential() # Initializing Sequential Model\n",
        "model.add(Embedding(voc_size,emb_vec_feature,input_length=sent_len)) # Adding embedding layer of vocab size * output vector size\n",
        "model.add(LSTM(100)) # Adding LSTM layer with 100 neurons\n",
        "model.add(Dense(1, activation=\"sigmoid\")) # Output layer with 1 neuron having sigmoid to tell probability of each class\n",
        "# Compile model with loss funtion Binary crossentropy and adam optimizer to minimize losses\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# Summary of model\n",
        "print(model.summary())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 20, 40)            200000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               56400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 256,501\n",
            "Trainable params: 256,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xMszY-TcGYw"
      },
      "source": [
        "# Converting vectors into array\n",
        "X_final = np.array(emb)\n",
        "y_final = np.array(y)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-cLLQX1j8OB"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final,y_final, test_size=0.33, random_state=42)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28MwzmgMkptP",
        "outputId": "53be78f1-2dd3-42fb-b993-639eecdf70b2"
      },
      "source": [
        "# Model Training\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10,batch_size=64)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "192/192 [==============================] - 8s 35ms/step - loss: 0.4912 - accuracy: 0.7404 - val_loss: 0.2154 - val_accuracy: 0.9107\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.1391 - accuracy: 0.9458 - val_loss: 0.1918 - val_accuracy: 0.9210\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.0925 - accuracy: 0.9653 - val_loss: 0.2171 - val_accuracy: 0.9168\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0769 - accuracy: 0.9752 - val_loss: 0.2319 - val_accuracy: 0.9104\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.0525 - accuracy: 0.9845 - val_loss: 0.2951 - val_accuracy: 0.9130\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0389 - accuracy: 0.9880 - val_loss: 0.3098 - val_accuracy: 0.9092\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 6s 31ms/step - loss: 0.0272 - accuracy: 0.9927 - val_loss: 0.3766 - val_accuracy: 0.9019\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.4064 - val_accuracy: 0.9092\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.4852 - val_accuracy: 0.9024\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.5755 - val_accuracy: 0.9077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fea3c979470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnpZzLtDy57r"
      },
      "source": [
        "# Dropout\n",
        "A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. This is called dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n",
        "\n",
        "Here, I'm just showing how to add dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PbtRBXslAJu",
        "outputId": "7ce4e134-9cfa-4651-e5e7-f45fc2524ed5"
      },
      "source": [
        "# Adding Dropout\n",
        "from keras.layers import Dropout\n",
        "# model\n",
        "emb_vec_feature1=40\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(voc_size,emb_vec_feature1,input_length=sent_len))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(LSTM(100))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(1, activation=\"sigmoid\"))\n",
        "model1.compile(loss='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model1.summary())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 20, 40)            200000    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 20, 40)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               56400     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 256,501\n",
            "Trainable params: 256,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FqEVmVaoIkw"
      },
      "source": [
        "X_final1 = np.array(emb1)\n",
        "y_final1 = np.array(y)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOpzfUIsoUcg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_final1,y_final1, test_size=0.33, random_state=42)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_p29NFsoc7J",
        "outputId": "2bcf3a91-c16b-4c78-d698-49e2b83265b7"
      },
      "source": [
        "model1.fit(X_train1, y_train1, validation_data=(X_test1, y_test1), epochs=10,batch_size=64)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "192/192 [==============================] - 8s 35ms/step - loss: 0.4909 - accuracy: 0.7530 - val_loss: 0.1986 - val_accuracy: 0.9132\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.1494 - accuracy: 0.9397 - val_loss: 0.2000 - val_accuracy: 0.9206\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.1057 - accuracy: 0.9624 - val_loss: 0.2251 - val_accuracy: 0.9155\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.0756 - accuracy: 0.9745 - val_loss: 0.2639 - val_accuracy: 0.9160\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.0602 - accuracy: 0.9797 - val_loss: 0.2758 - val_accuracy: 0.9118\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0433 - accuracy: 0.9853 - val_loss: 0.3473 - val_accuracy: 0.9034\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.0351 - accuracy: 0.9885 - val_loss: 0.3837 - val_accuracy: 0.9056\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 6s 33ms/step - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.3799 - val_accuracy: 0.9080\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0224 - accuracy: 0.9926 - val_loss: 0.4283 - val_accuracy: 0.9099\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 6s 32ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.4683 - val_accuracy: 0.9006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fea3c41b208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUJ9GAsyle_h",
        "outputId": "96ff8bda-9c29-4131-d0f4-27ca19242789"
      },
      "source": [
        "# Predict labels for testing set\n",
        "y_pred = model.predict_classes(X_test)\n",
        "y_pred1 = model1.predict_classes(X_test1)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBR-_aVDlqks",
        "outputId": "63878a38-cf8d-4b8e-cbfc-2b5b9a896b5c"
      },
      "source": [
        "# Drawing Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(confusion_matrix(y_test1,y_pred1))\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3057  362]\n",
            " [ 195 2421]]\n",
            "[[3130  289]\n",
            " [ 311 2305]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMf1WDu0l1dG",
        "outputId": "2597d933-ca51-4291-ca84-0043a5f0eef1"
      },
      "source": [
        "# Accuracy Score\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Model:\",accuracy_score(y_test,y_pred))\n",
        "print()\n",
        "print(\"Model with Dropout\",accuracy_score(y_test1,y_pred1))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: 0.907705053852527\n",
            "\n",
            "Model with Dropout 0.9005799502899752\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}